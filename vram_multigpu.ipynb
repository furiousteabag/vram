{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5261516b-0d0c-4f4c-afbb-23c1db13d673",
   "metadata": {},
   "source": [
    "pip install transformers nvidia-ml-py3 einops ipyexperiments accelerate sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d0272a-78b0-48ac-b6d1-e7b57dc01650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from ipyexperiments import IPyExperimentsPytorch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef691ce6-276d-4d79-855a-58ad721b5af0",
   "metadata": {},
   "source": [
    "# Run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cb2ead-719b-4e17-8558-3c1ae4bb0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_name_or_path = \"NousResearch/Llama-2-13b-hf\" # microsoft/phi-1_5, microsoft/phi-2, NousResearch/Llama-2-7b-hf, mistralai/Mistral-7B-v0.1, gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "dtype = torch.float32\n",
    "mixed_precision_training = True\n",
    "bs = 4\n",
    "seq_length = 2048\n",
    "get_optimizer = lambda parameters: torch.optim.SGD(parameters, lr=0.1, momentum=0.9) # SGD(parameters, lr=0.1), SGD(parameters, lr=0.1, momentum=0.9), AdamW(parameters, lr=0.1)\n",
    "\n",
    "if mixed_precision_training:\n",
    "    assert dtype == torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d48830c-cd59-489b-b500-459eb647c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA kernels VRAM: 716 MiB\n",
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 3090 (24576 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:     Used     Free    Total        Util\n",
      "CPU:    1,727  123,637  128,658 MB   1.34% \n",
      "GPU:    1,035   23,540   24,576 MB   4.21% \n",
      "\n",
      "\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.000\n",
      "･ CPU:          0          0      1,727 MB |\n",
      "･ GPU:          0          0      1,035 MB |\n"
     ]
    }
   ],
   "source": [
    "n_bytes_per_param = 2 if dtype in (torch.float16, torch.bfloat16) else 4\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "get_vram = lambda: pynvml.nvmlDeviceGetMemoryInfo(handle).used / 2**20 # MiB\n",
    "\n",
    "start_vram = get_vram()\n",
    "\n",
    "# Initializing CUDA kernels\n",
    "a = torch.ones((1,1)).to(device); del a; gc.collect(); torch.cuda.empty_cache()\n",
    "cuda_kernels_vram = get_vram() - start_vram\n",
    "print(f\"CUDA kernels VRAM: {cuda_kernels_vram:.0f} MiB\")\n",
    "\n",
    "exp = IPyExperimentsPytorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83e0211-63ae-4536-b188-0f7c83cabf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 19 13:24:10 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   30C    P2              97W / 370W |    718MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        On  | 00000000:82:00.0 Off |                  N/A |\n",
      "|  0%   24C    P8              23W / 370W |      5MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090        On  | 00000000:C1:00.0 Off |                  N/A |\n",
      "|  0%   25C    P8              24W / 370W |      5MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        On  | 00000000:C2:00.0 Off |                  N/A |\n",
      "|  0%   23C    P8              26W / 370W |      5MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.412\n",
      "･ CPU:          2          0      1,730 MB |\n",
      "･ GPU:          0          0      1,035 MB |\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689d757-e854-45b8-a35d-3e6e31994b83",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56bcc214-a1f6-43f5-836c-157be2afd2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb369e32ea04977a0dddd68e2b06f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "big_modeling.py:411 -       dispatch_model() | Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"NousResearch/Llama-2-13b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 5120,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 13824,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 40,\n",
      "  \"num_hidden_layers\": 40,\n",
      "  \"num_key_value_heads\": 40,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "===========================================================================\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-39): 40 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      ")\n",
      "===========================================================================\n",
      "Number of parameters: 13.016 B (13015864320)\n",
      "Number of parameters: 13.058 B (13057809920)\n",
      "Model VRAM usage: 22300 MiB (expected 49812 MiB, error -123.4 %)\n",
      "===========================================================================\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:14.996\n",
      "･ CPU:      9,267      5,880     15,470 MB |\n",
      "･ GPU:     10,366          0     23,335 MB |\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype, trust_remote_code=True, device_map=\"auto\")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "n_training_parameters = sum(p.numel() for p in model.parameters())\n",
    "n_parameters = sum(p.numel() for p in model.parameters()) + sum(p.numel() for p in model.buffers())\n",
    "model_estimated_vram = n_parameters * n_bytes_per_param / 2**20\n",
    "model_actual_vram = get_vram() - cuda_kernels_vram - start_vram\n",
    "\n",
    "print(model.config)\n",
    "print(\"=\" * 75)\n",
    "print(model)\n",
    "print(\"=\" * 75)\n",
    "print(f\"Number of parameters: {(n_training_parameters / 1e9):.3f} B ({n_training_parameters})\")\n",
    "print(f\"Number of parameters: {(n_parameters / 1e9):.3f} B ({n_parameters})\")\n",
    "print(f\"Model VRAM usage: {(model_actual_vram):.0f} MiB (expected {(model_estimated_vram):.0f} MiB, error {((model_actual_vram - model_estimated_vram) * 100 / model_actual_vram):.1f} %)\")\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69cd292a-9e1b-4279-968d-c60099c07063",
   "metadata": {},
   "source": [
    "bs = 128\n",
    "seq_length = 8192\n",
    "\n",
    "batch_vram = 3 * bs * seq_length * 8 # 3 for input_ids, attention_masks and labels; 8 for each i64\n",
    "print(f\"For batch of {bs} items with a sequence length of {seq_length} it will consume {batch_vram / 2**20} MiB VRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c62e7c99-0480-41b1-8a36-715a97a72854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 19 13:24:30 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   32C    P2              97W / 370W |  12276MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        On  | 00000000:82:00.0 Off |                  N/A |\n",
      "|  0%   34C    P2              94W / 370W |  14090MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090        On  | 00000000:C1:00.0 Off |                  N/A |\n",
      "|  0%   37C    P2              99W / 370W |  14090MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        On  | 00000000:C2:00.0 Off |                  N/A |\n",
      "|  0%   33C    P2              95W / 370W |  12276MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.452\n",
      "･ CPU:          0          0      3,896 MB |\n",
      "･ GPU:          0          0     12,593 MB |\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5762f438-77e1-4c6d-a3c9-7eaeab21dd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.004\n",
      "･ CPU:          0          0      3,896 MB |\n",
      "･ GPU:          0          0     12,593 MB |\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.randint(0, len(tokenizer), (bs, seq_length)).to(device)\n",
    "attention_mask = torch.ones((bs, seq_length)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b004db1-7540-47c7-a498-aa8f43b910b6",
   "metadata": {},
   "source": [
    "# Inference forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b09e07-9fb7-4082-9bd5-e12b15d18fb0",
   "metadata": {},
   "source": [
    "## warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c077eb8-6819-4ebf-86ea-c056b7824db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference warmup took: 376 MiB\n",
      "===========================================================================\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:16.860\n",
      "･ CPU:      2,306          0      6,202 MB |\n",
      "･ GPU:        376      6,786     12,969 MB |\n"
     ]
    }
   ],
   "source": [
    "_ = model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "del out; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "inference_warmup = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "warmup = inference_warmup\n",
    "print(f\"Inference warmup took: {inference_warmup:.0f} MiB\")\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774a8bb3-dd74-460d-a5be-3fec5a82d668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 19 13:24:48 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   34C    P2              99W / 370W |  12652MiB / 24576MiB |     18%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        On  | 00000000:82:00.0 Off |                  N/A |\n",
      "|  0%   36C    P2              97W / 370W |  14450MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090        On  | 00000000:C1:00.0 Off |                  N/A |\n",
      "|  0%   41C    P2             101W / 370W |  14450MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        On  | 00000000:C2:00.0 Off |                  N/A |\n",
      "|  0%   43C    P2             194W / 370W |  12656MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.488\n",
      "･ CPU:          0          0      6,202 MB |\n",
      "･ GPU:          0          0     12,969 MB |\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3ac59-bb1f-4d8d-a515-d0231d88df10",
   "metadata": {},
   "source": [
    "## actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "733503d2-e9de-4122-93a2-eb7ca86f87fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out tensor dtype: torch.float32\n",
      "Tue Dec 19 13:25:03 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   34C    P2              98W / 370W |  19454MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        On  | 00000000:82:00.0 Off |                  N/A |\n",
      "|  0%   36C    P2              96W / 370W |  21156MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090        On  | 00000000:C1:00.0 Off |                  N/A |\n",
      "|  0%   43C    P2             101W / 370W |  21156MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        On  | 00000000:C2:00.0 Off |                  N/A |\n",
      "|  0%   46C    P2             230W / 370W |  19362MiB / 24576MiB |     10%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Tue Dec 19 13:25:03 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   34C    P2              99W / 370W |  15212MiB / 24576MiB |     24%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        On  | 00000000:82:00.0 Off |                  N/A |\n",
      "|  0%   36C    P2              97W / 370W |  14450MiB / 24576MiB |     14%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090        On  | 00000000:C1:00.0 Off |                  N/A |\n",
      "|  0%   42C    P2             101W / 370W |  14450MiB / 24576MiB |      9%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        On  | 00000000:C2:00.0 Off |                  N/A |\n",
      "|  0%   41C    P2             109W / 370W |  12656MiB / 24576MiB |     32%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Total forward pass VRAM usage: 6802 MiB\n",
      "Output tensor with bs 4, seq length 2048 and emb size 32000 VRAM usage: 2560 MiB (expected 1000 MiB)\n",
      "Activations VRAM usage: 4242 MiB\n",
      "===========================================================================\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:15.576\n",
      "･ CPU:          0          0      6,203 MB |\n",
      "･ GPU:          0      6,802     12,969 MB |\n"
     ]
    }
   ],
   "source": [
    "_ = model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)[0]\n",
    "    # probs = F.softmax(out.logits[:, -1, :], dim=-1) # for inference we need probabilities only over the last token; omit this as it is very small\n",
    "\n",
    "out_bs, out_sequence_length, out_embedding_size = out.shape\n",
    "n_bytes_per_param_out = 2 if out.dtype in (torch.float16, torch.bfloat16) else 4\n",
    "output_estimated_vram = out_bs * out_sequence_length * out_embedding_size * n_bytes_per_param_out / 2**20\n",
    "print(f\"Out tensor dtype: {out.dtype}\")\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "total_forward_pass_vram = get_vram() - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "output_vram = get_vram() - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "del out; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "activations_actual_vram = total_forward_pass_vram - output_vram\n",
    "\n",
    "print(f\"Total forward pass VRAM usage: {total_forward_pass_vram:.0f} MiB\")\n",
    "print(f\"Output tensor with bs {out_bs}, seq length {out_sequence_length} and emb size {out_embedding_size} VRAM usage: {output_vram:.0f} MiB (expected {output_estimated_vram:.0f} MiB)\")\n",
    "print(f\"Activations VRAM usage: {activations_actual_vram:.0f} MiB\")\n",
    "#print(torch.cuda.memory_summary())\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "096f1c28-09ff-4848-b896-6f08a9ec5f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 19 13:25:04 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   34C    P2              99W / 370W |  12652MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        On  | 00000000:82:00.0 Off |                  N/A |\n",
      "|  0%   36C    P2              98W / 370W |  14450MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090        On  | 00000000:C1:00.0 Off |                  N/A |\n",
      "|  0%   42C    P2             102W / 370W |  14450MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        On  | 00000000:C2:00.0 Off |                  N/A |\n",
      "|  0%   39C    P2             100W / 370W |  12656MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.485\n",
      "･ CPU:          0          0      6,203 MB |\n",
      "･ GPU:          0          0     12,969 MB |\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f8bac-c9a4-42b5-bad8-3f5e41fba771",
   "metadata": {},
   "source": [
    "# Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e54e3c-4020-4bdf-9ae9-68c12b72d657",
   "metadata": {},
   "source": [
    "## warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f5865-a021-40bd-82ba-be2ef3911cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.train()\n",
    "optimizer = get_optimizer(model.parameters())\n",
    "\n",
    "with torch.autocast(device_type=device.type, dtype=torch.bfloat16, enabled=mixed_precision_training):\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    probs = F.softmax(out.logits, dim=-1)\n",
    "    loss = F.cross_entropy(probs.permute(0, 2, 1), input_ids)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "del out\n",
    "del probs\n",
    "del loss\n",
    "del optimizer\n",
    "\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "train_warmup = get_vram() - inference_warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "warmup += train_warmup\n",
    "print(f\"Train warmup took: {train_warmup:.0f} MiB\")\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72715b72-730a-4032-b7fe-88b6ad95a070",
   "metadata": {},
   "source": [
    "## actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b4701-8c34-4178-98b6-b255ee9f320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.train()\n",
    "optimizer = get_optimizer(model.parameters())\n",
    "#scaler = torch.cuda.amp.GradScaler(enabled=mixed_precision_training) # scaler is not needed with bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc1739-1546-4c4e-af83-c8c6c9f8318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(device_type=device.type, dtype=torch.bfloat16, enabled=mixed_precision_training):\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    train_forward_pass_vram = get_vram() - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "    \n",
    "    probs = F.softmax(out.logits, dim=-1)\n",
    "    probs_vram = get_vram() - train_forward_pass_vram - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "    \n",
    "    loss = F.cross_entropy(probs.permute(0, 2, 1), input_ids) # mapping tokens into themselves\n",
    "    loss_calculation_vram = get_vram() - probs_vram - train_forward_pass_vram - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "#scaler.scale(loss).backward()\n",
    "#scaler.step(optimizer)\n",
    "#scaler.update()\n",
    "backward_vram = get_vram() - loss_calculation_vram - probs_vram - train_forward_pass_vram - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "\n",
    "total_train_forward_pass_vram = train_forward_pass_vram + probs_vram + loss_calculation_vram + backward_vram\n",
    "\n",
    "print(f\"Model gradients type: {next(model.parameters()).grad.dtype}\")\n",
    "print(f\"Total train forward pass VRAM usage: {total_train_forward_pass_vram:.0f} MiB\" + (f\" (expect {(n_parameters * 2 / 2**20):.0f} MiB of these to be for fp16 weights copy)\" if mixed_precision_training else \"\"))\n",
    "#print(f\"Actual probs tensor VRAM usage: {probs_vram:.0f} MiB\")\n",
    "#print(f\"Loss calculation VRAM usage: {loss_calculation_vram:.0f} MiB\")\n",
    "#print(f\"Backward calculation VRAM usage: {backward_vram:.0f} MiB\")\n",
    "\n",
    "del out\n",
    "del probs\n",
    "del loss\n",
    "gc.collect(); torch.cuda.empty_cache() # calling `free` on allocated memory for activations and outputs\n",
    "\n",
    "gradients_optimizer_total_vram = get_vram() - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "optimizer.zero_grad(set_to_none=True); gc.collect(); torch.cuda.empty_cache()\n",
    "optimizer_vram = get_vram() - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "del optimizer; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "gradients_vram = gradients_optimizer_total_vram - optimizer_vram\n",
    "print(f\"Gradients VRAM usage: {gradients_vram:.0f} MiB (trainable params were {(n_training_parameters * n_bytes_per_param / 2**20):.0f} MiB)\")\n",
    "print(f\"Optimizer states VRAM usage: {optimizer_vram:.0f} MiB\")\n",
    "print(f\"Activations VRAM usage: {(total_train_forward_pass_vram - (n_parameters * 2 / 2**20 if mixed_precision_training else 0) - output_estimated_vram * 2 - gradients_vram - optimizer_vram):.0f} MiB\")\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd62ab9-22e3-4e84-943e-82f4b63762a1",
   "metadata": {},
   "source": [
    "# Estimation activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca072747-a715-4827-a4a1-3335b9c844a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bytes_per_param = 2 if mixed_precision_training or dtype in (torch.float16, torch.bfloat16) else 4\n",
    "\n",
    "hidden_size = model.config.hidden_size\n",
    "num_attention_heads = model.config.num_attention_heads\n",
    "num_key_value_heads = model.config.num_key_value_heads if hasattr(model.config, \"num_key_value_heads\") else model.config.num_attention_heads # different from num_attention_heads in case of GQA\n",
    "intermediate_size = model.config.intermediate_size if hasattr(model.config, \"intermediate_size\") else 4 * model.config.hidden_size # MLP projection\n",
    "num_hidden_layers = model.config.num_hidden_layers\n",
    "head_dim = hidden_size // num_attention_heads\n",
    "print(f\"Calculating size of activation for single block with:\\nbatch size {bs}\\nseq length {seq_length}\\nhidden size {hidden_size}\\nnum attention heads {num_attention_heads}\\nnum key value heads {num_key_value_heads}\\nintermediate size {intermediate_size}\\nhead dim {head_dim}\\nnum hidden layers {num_hidden_layers}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "attention_input      = n_bytes_per_param * bs * seq_length * hidden_size\n",
    "q                    = n_bytes_per_param * bs * seq_length * head_dim * num_attention_heads # for Q @ K.T\n",
    "k                    = n_bytes_per_param * bs * seq_length * head_dim * num_key_value_heads # num_key_value_heads might be different from num_attention_heads in case of GQA\n",
    "softmax_output       = n_bytes_per_param * bs * num_attention_heads * seq_length ** 2 # to multiply with V\n",
    "softmax_dropout_mask = 1                 * bs * num_attention_heads * seq_length ** 2 # single byte per elem\n",
    "dropout_output       = n_bytes_per_param * bs * num_attention_heads * seq_length ** 2\n",
    "v                    = n_bytes_per_param * bs * seq_length * head_dim * num_key_value_heads\n",
    "out_proj_input       = n_bytes_per_param * bs * seq_length * num_attention_heads * head_dim\n",
    "attention_dropout    = 1                 * bs * seq_length * hidden_size\n",
    "#attention_block = attention_input + q + k + softmax_output + v + out_proj_input\n",
    "attention_block = attention_input + q + k + softmax_output + v + out_proj_input + softmax_dropout_mask + dropout_output + attention_dropout\n",
    "\n",
    "mlp_input        = n_bytes_per_param * bs * seq_length * hidden_size\n",
    "activation_input = n_bytes_per_param * bs * seq_length * intermediate_size # SiLU\n",
    "down_proj_input  = n_bytes_per_param * bs * seq_length * intermediate_size\n",
    "dropout_mask     = 1                 * bs * seq_length * hidden_size # single byte per elem\n",
    "#mlp_block = mlp_input + activation_input + down_proj_input\n",
    "mlp_block = mlp_input + activation_input + down_proj_input + dropout_mask\n",
    "\n",
    "layer_norms = n_bytes_per_param * bs * seq_length * hidden_size * 2 # 2 layer norms\n",
    "\n",
    "layer = attention_block + mlp_block + layer_norms\n",
    "print(f\"Single layer (out of {num_hidden_layers}) estimated activations VRAM usage: {layer // 2**20} MiB\")\n",
    "print(f\"All layers estimated activations VRAM usage: {layer * num_hidden_layers // 2**20} MiB\")\n",
    "print(f\"Estimated activations on inference forward pass VRAM usage (softmax output + v): {(softmax_output + v) // 2**20} MiB\")\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70deac-a86a-403d-a21b-097e77e932fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/2205.05198.pdf\n",
    "\n",
    "def calculate_attention_block():\n",
    "     return 11 * seq_length * bs * hidden_size + 5 * num_attention_heads * seq_length ** 2 * bs\n",
    "\n",
    "def calculate_mlp_block():\n",
    "     return 19 * seq_length * bs * hidden_size\n",
    "\n",
    "def calculate_layernorms():\n",
    "    return 4 * seq_length * bs * hidden_size\n",
    "\n",
    "def calculate_per_layer():\n",
    "    return seq_length * bs * hidden_size * (34 + 5 * num_attention_heads * seq_length / hidden_size)\n",
    "\n",
    "assert calculate_attention_block() + calculate_mlp_block() + calculate_layernorms() == calculate_per_layer() == layer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23628603-07f8-4f3f-9731-018939acf519",
   "metadata": {},
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
