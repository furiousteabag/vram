{
 "cells": [
  {
   "cell_type": "raw",
   "id": "574920d7-8053-4f00-b775-b2708aea45c3",
   "metadata": {},
   "source": [
    "pip install transformers nvidia-ml-py3 einops ipyexperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d0272a-78b0-48ac-b6d1-e7b57dc01650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pynvml\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from ipyexperiments import IPyExperimentsPytorch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef691ce6-276d-4d79-855a-58ad721b5af0",
   "metadata": {},
   "source": [
    "# Run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cb2ead-719b-4e17-8558-3c1ae4bb0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_name_or_path = \"microsoft/phi-1_5\" # microsoft/phi-1_5, microsoft/phi-2, NousResearch/Llama-2-7b-hf, mistralai/Mistral-7B-v0.1, gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "dtype = torch.float32\n",
    "mixed_precision_training = True\n",
    "bs = 4\n",
    "seq_length = 512\n",
    "get_optimizer = lambda parameters: torch.optim.SGD(parameters, lr=0.1, momentum=0.9) # SGD(parameters, lr=0.1), SGD(parameters, lr=0.1, momentum=0.9), AdamW(parameters, lr=0.1)\n",
    "\n",
    "if mixed_precision_training:\n",
    "    assert dtype == torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d48830c-cd59-489b-b500-459eb647c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA kernels VRAM: 911 MiB\n",
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA A100-PCIE-40GB (40960 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:     Used     Free    Total        Util\n",
      "CPU:    1,724  242,815  257,711 MB   0.67% \n",
      "GPU:    1,536   39,423   40,960 MB   3.75% \n",
      "\n",
      "\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.000\n",
      "･ CPU:          0          0      1,724 MB |\n",
      "･ GPU:          0          0      1,536 MB |\n"
     ]
    }
   ],
   "source": [
    "n_bytes_per_param = 2 if dtype in (torch.float16, torch.bfloat16) else 4\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "get_vram = lambda: pynvml.nvmlDeviceGetMemoryInfo(handle).used / 2**20 # MiB\n",
    "\n",
    "start_vram = get_vram()\n",
    "\n",
    "# Initializing CUDA kernels\n",
    "a = torch.ones((1,1)).to(device); del a; gc.collect(); torch.cuda.empty_cache()\n",
    "cuda_kernels_vram = get_vram() - start_vram\n",
    "print(f\"CUDA kernels VRAM: {cuda_kernels_vram:.0f} MiB\")\n",
    "\n",
    "exp = IPyExperimentsPytorch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689d757-e854-45b8-a35d-3e6e31994b83",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bcc214-a1f6-43f5-836c-157be2afd2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiConfig {\n",
      "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"PhiForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_phi.PhiConfig\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_phi.PhiForCausalLM\"\n",
      "  },\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"flash_attn\": false,\n",
      "  \"flash_rotary\": false,\n",
      "  \"fused_dense\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"phi-msft\",\n",
      "  \"n_embd\": 2048,\n",
      "  \"n_head\": 32,\n",
      "  \"n_head_kv\": null,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary_dim\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.36.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "===========================================================================\n",
      "PhiForCausalLM(\n",
      "  (transformer): PhiModel(\n",
      "    (embd): Embedding(\n",
      "      (wte): Embedding(51200, 2048)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x ParallelBlock(\n",
      "        (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (mixer): MHA(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (inner_attn): SelfAttention(\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (inner_cross_attn): CrossAttention(\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): CausalLMHead(\n",
      "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
      "  )\n",
      "  (loss): CausalLMLoss(\n",
      "    (loss_fct): CrossEntropyLoss()\n",
      "  )\n",
      ")\n",
      "===========================================================================\n",
      "Number of parameters: 1.418 B (1418271104)\n",
      "Model VRAM usage: 5412 MiB (expected 5410 MiB, error 0.0 %)\n",
      "===========================================================================\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:27.409\n",
      "･ CPU:        572      7,531      2,297 MB |\n",
      "･ GPU:      5,412          0      6,948 MB |\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype, trust_remote_code=True).to(device)\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "n_training_parameters = sum(p.numel() for p in model.parameters())\n",
    "n_parameters = sum(p.numel() for p in model.parameters()) + sum(p.numel() for p in model.buffers())\n",
    "model_estimated_vram = n_parameters * n_bytes_per_param / 2**20\n",
    "model_actual_vram = get_vram() - cuda_kernels_vram - start_vram\n",
    "\n",
    "print(model.config)\n",
    "print(\"=\" * 75)\n",
    "print(model)\n",
    "print(\"=\" * 75)\n",
    "print(f\"Number of parameters: {(n_parameters / 1e9):.3f} B ({n_parameters})\")\n",
    "print(f\"Model VRAM usage: {(model_actual_vram):.0f} MiB (expected {(model_estimated_vram):.0f} MiB, error {((model_actual_vram - model_estimated_vram) * 100 / model_actual_vram):.1f} %)\")\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69cd292a-9e1b-4279-968d-c60099c07063",
   "metadata": {},
   "source": [
    "bs = 128\n",
    "seq_length = 8192\n",
    "\n",
    "batch_vram = 3 * bs * seq_length * 8 # 3 for input_ids, attention_masks and labels; 8 for each i64\n",
    "print(f\"For batch of {bs} items with a sequence length of {seq_length} it will consume {batch_vram / 2**20} MiB VRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5762f438-77e1-4c6d-a3c9-7eaeab21dd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.007\n",
      "･ CPU:          0          0      2,297 MB |\n",
      "･ GPU:          0          0      6,948 MB |\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.randint(0, len(tokenizer), (bs, seq_length)).to(device)\n",
    "attention_mask = torch.ones((bs, seq_length)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b004db1-7540-47c7-a498-aa8f43b910b6",
   "metadata": {},
   "source": [
    "# Inference forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b09e07-9fb7-4082-9bd5-e12b15d18fb0",
   "metadata": {},
   "source": [
    "## warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c077eb8-6819-4ebf-86ea-c056b7824db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference warmup took: 382 MiB\n",
      "===========================================================================\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:01.732\n",
      "･ CPU:        754          0      3,051 MB |\n",
      "･ GPU:        382        966      7,330 MB |\n"
     ]
    }
   ],
   "source": [
    "_ = model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "del out; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "inference_warmup = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "warmup = inference_warmup\n",
    "print(f\"Inference warmup took: {inference_warmup:.0f} MiB\")\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3ac59-bb1f-4d8d-a515-d0231d88df10",
   "metadata": {},
   "source": [
    "## actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "733503d2-e9de-4122-93a2-eb7ca86f87fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out tensor dtype: torch.float32\n",
      "Total forward pass VRAM usage: 966 MiB\n",
      "Output tensor with bs 4, seq length 512 and emb size 51200 VRAM usage: 400 MiB (expected 400 MiB)\n",
      "Activations VRAM usage: 566 MiB\n",
      "===========================================================================\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.483\n",
      "･ CPU:          0          0      3,051 MB |\n",
      "･ GPU:          0        966      7,330 MB |\n"
     ]
    }
   ],
   "source": [
    "_ = model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    # probs = F.softmax(out.logits[:, -1, :], dim=-1) # for inference we need probabilities only over the last token; omit this as it is very small\n",
    "\n",
    "out_bs, out_sequence_length, out_embedding_size = out.logits.shape\n",
    "n_bytes_per_param_out = 2 if out.logits.dtype in (torch.float16, torch.bfloat16) else 4\n",
    "output_estimated_vram = out_bs * out_sequence_length * out_embedding_size * n_bytes_per_param_out / 2**20\n",
    "print(f\"Out tensor dtype: {out.logits.dtype}\")\n",
    "\n",
    "total_forward_pass_vram = get_vram() - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "output_vram = get_vram() - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "del out; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "activations_actual_vram = total_forward_pass_vram - output_vram\n",
    "\n",
    "print(f\"Total forward pass VRAM usage: {total_forward_pass_vram:.0f} MiB\")\n",
    "print(f\"Output tensor with bs {out_bs}, seq length {out_sequence_length} and emb size {out_embedding_size} VRAM usage: {output_vram:.0f} MiB (expected {output_estimated_vram:.0f} MiB)\")\n",
    "print(f\"Activations VRAM usage: {activations_actual_vram:.0f} MiB\")\n",
    "#print(torch.cuda.memory_summary())\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f8bac-c9a4-42b5-bad8-3f5e41fba771",
   "metadata": {},
   "source": [
    "# Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e54e3c-4020-4bdf-9ae9-68c12b72d657",
   "metadata": {},
   "source": [
    "## warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b3f5865-a021-40bd-82ba-be2ef3911cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train warmup took: 74 MiB\n",
      "===========================================================================\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.594\n",
      "･ CPU:          5          0      3,057 MB |\n",
      "･ GPU:         74     17,850      7,404 MB |\n"
     ]
    }
   ],
   "source": [
    "_ = model.train()\n",
    "optimizer = get_optimizer(model.parameters())\n",
    "\n",
    "with torch.autocast(device_type=device.type, dtype=torch.bfloat16, enabled=mixed_precision_training):\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    probs = F.softmax(out.logits, dim=-1)\n",
    "    loss = F.cross_entropy(probs.permute(0, 2, 1), input_ids)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "del out\n",
    "del probs\n",
    "del loss\n",
    "del optimizer\n",
    "\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "train_warmup = get_vram() - inference_warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "warmup += train_warmup\n",
    "print(f\"Train warmup took: {train_warmup:.0f} MiB\")\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72715b72-730a-4032-b7fe-88b6ad95a070",
   "metadata": {},
   "source": [
    "## actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923b4701-8c34-4178-98b6-b255ee9f320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.003\n",
      "･ CPU:          0          0      3,057 MB |\n",
      "･ GPU:          0          0      7,404 MB |\n"
     ]
    }
   ],
   "source": [
    "_ = model.train()\n",
    "optimizer = get_optimizer(model.parameters())\n",
    "#scaler = torch.cuda.amp.GradScaler(enabled=mixed_precision_training) # scaler is not needed with bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24bc1739-1546-4c4e-af83-c8c6c9f8318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gradients type: torch.float32\n",
      "Total train forward pass VRAM usage: 17862 MiB (expect 2705 MiB of these to be for fp16 weights copy)\n",
      "Gradients VRAM usage: 5104 MiB (trainable params were 5410 MiB)\n",
      "Optimizer states VRAM usage: 5850 MiB\n",
      "Activations VRAM usage: 3403 MiB\n",
      "===========================================================================\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.909\n",
      "･ CPU:          0          0      3,057 MB |\n",
      "･ GPU:          0     17,862      7,404 MB |\n"
     ]
    }
   ],
   "source": [
    "with torch.autocast(device_type=device.type, dtype=torch.bfloat16, enabled=mixed_precision_training):\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    train_forward_pass_vram = get_vram() - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "    \n",
    "    probs = F.softmax(out.logits, dim=-1)\n",
    "    probs_vram = get_vram() - train_forward_pass_vram - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "    \n",
    "    loss = F.cross_entropy(probs.permute(0, 2, 1), input_ids) # mapping tokens into themselves\n",
    "    loss_calculation_vram = get_vram() - probs_vram - train_forward_pass_vram - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "#scaler.scale(loss).backward()\n",
    "#scaler.step(optimizer)\n",
    "#scaler.update()\n",
    "backward_vram = get_vram() - loss_calculation_vram - probs_vram - train_forward_pass_vram - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "\n",
    "total_train_forward_pass_vram = train_forward_pass_vram + probs_vram + loss_calculation_vram + backward_vram\n",
    "\n",
    "print(f\"Model gradients type: {next(model.parameters()).grad.dtype}\")\n",
    "print(f\"Total train forward pass VRAM usage: {total_train_forward_pass_vram:.0f} MiB\" + (f\" (expect {(n_parameters * 2 / 2**20):.0f} MiB of these to be for fp16 weights copy)\" if mixed_precision_training else \"\"))\n",
    "#print(f\"Actual probs tensor VRAM usage: {probs_vram:.0f} MiB\")\n",
    "#print(f\"Loss calculation VRAM usage: {loss_calculation_vram:.0f} MiB\")\n",
    "#print(f\"Backward calculation VRAM usage: {backward_vram:.0f} MiB\")\n",
    "\n",
    "del out\n",
    "del probs\n",
    "del loss\n",
    "gc.collect(); torch.cuda.empty_cache() # calling `free` on allocated memory for activations and outputs\n",
    "\n",
    "gradients_optimizer_total_vram = get_vram() - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "optimizer.zero_grad(set_to_none=True); gc.collect(); torch.cuda.empty_cache()\n",
    "optimizer_vram = get_vram() - warmup - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "del optimizer; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "gradients_vram = gradients_optimizer_total_vram - optimizer_vram\n",
    "print(f\"Gradients VRAM usage: {gradients_vram:.0f} MiB (trainable params were {(n_training_parameters * n_bytes_per_param / 2**20):.0f} MiB)\")\n",
    "print(f\"Optimizer states VRAM usage: {optimizer_vram:.0f} MiB\")\n",
    "print(f\"Activations VRAM usage: {(total_train_forward_pass_vram - (n_parameters * 2 / 2**20 if mixed_precision_training else 0) - output_estimated_vram * 2 - gradients_vram - optimizer_vram):.0f} MiB\")\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd62ab9-22e3-4e84-943e-82f4b63762a1",
   "metadata": {},
   "source": [
    "# Estimation activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca072747-a715-4827-a4a1-3335b9c844a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating size of activation for single block with:\n",
      "batch size 4\n",
      "seq length 512\n",
      "hidden size 2048\n",
      "num attention heads 32\n",
      "num key value heads 32\n",
      "intermediate size 8192\n",
      "head dim 64\n",
      "num hidden layers 24\n",
      "===========================================================================\n",
      "Single layer (out of 24) estimated activations VRAM usage: 296 MiB\n",
      "All layers estimated activations VRAM usage: 7104 MiB\n",
      "Estimated activations on inference forward pass VRAM usage (softmax output + v): 72 MiB\n",
      "===========================================================================\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.003\n",
      "･ CPU:          0          0      3,057 MB |\n",
      "･ GPU:          0          0      7,404 MB |\n"
     ]
    }
   ],
   "source": [
    "n_bytes_per_param = 2 if mixed_precision_training or dtype in (torch.float16, torch.bfloat16) else 4\n",
    "\n",
    "hidden_size = model.config.hidden_size\n",
    "num_attention_heads = model.config.num_attention_heads\n",
    "num_key_value_heads = model.config.num_key_value_heads if hasattr(model.config, \"num_key_value_heads\") else model.config.num_attention_heads # different from num_attention_heads in case of GQA\n",
    "intermediate_size = model.config.intermediate_size if hasattr(model.config, \"intermediate_size\") else 4 * model.config.hidden_size # MLP projection\n",
    "num_hidden_layers = model.config.num_hidden_layers\n",
    "head_dim = hidden_size // num_attention_heads\n",
    "print(f\"Calculating size of activation for single block with:\\nbatch size {bs}\\nseq length {seq_length}\\nhidden size {hidden_size}\\nnum attention heads {num_attention_heads}\\nnum key value heads {num_key_value_heads}\\nintermediate size {intermediate_size}\\nhead dim {head_dim}\\nnum hidden layers {num_hidden_layers}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "attention_input      = n_bytes_per_param * bs * seq_length * hidden_size\n",
    "q                    = n_bytes_per_param * bs * seq_length * head_dim * num_attention_heads # for Q @ K.T\n",
    "k                    = n_bytes_per_param * bs * seq_length * head_dim * num_key_value_heads # num_key_value_heads might be different from num_attention_heads in case of GQA\n",
    "softmax_output       = n_bytes_per_param * bs * num_attention_heads * seq_length ** 2 # to multiply with V\n",
    "softmax_dropout_mask = 1                 * bs * num_attention_heads * seq_length ** 2 # single byte per elem\n",
    "dropout_output       = n_bytes_per_param * bs * num_attention_heads * seq_length ** 2\n",
    "v                    = n_bytes_per_param * bs * seq_length * head_dim * num_key_value_heads\n",
    "out_proj_input       = n_bytes_per_param * bs * seq_length * num_attention_heads * head_dim\n",
    "attention_dropout    = 1                 * bs * seq_length * hidden_size\n",
    "#attention_block = attention_input + q + k + softmax_output + v + out_proj_input\n",
    "attention_block = attention_input + q + k + softmax_output + v + out_proj_input + softmax_dropout_mask + dropout_output + attention_dropout\n",
    "\n",
    "mlp_input        = n_bytes_per_param * bs * seq_length * hidden_size\n",
    "activation_input = n_bytes_per_param * bs * seq_length * intermediate_size # SiLU\n",
    "down_proj_input  = n_bytes_per_param * bs * seq_length * intermediate_size\n",
    "dropout_mask     = 1                 * bs * seq_length * hidden_size # single byte per elem\n",
    "#mlp_block = mlp_input + activation_input + down_proj_input\n",
    "mlp_block = mlp_input + activation_input + down_proj_input + dropout_mask\n",
    "\n",
    "layer_norms = n_bytes_per_param * bs * seq_length * hidden_size * 2 # 2 layer norms\n",
    "\n",
    "layer = attention_block + mlp_block + layer_norms\n",
    "print(f\"Single layer (out of {num_hidden_layers}) estimated activations VRAM usage: {layer // 2**20} MiB\")\n",
    "print(f\"All layers estimated activations VRAM usage: {layer * num_hidden_layers // 2**20} MiB\")\n",
    "print(f\"Estimated activations on inference forward pass VRAM usage (softmax output + v): {(softmax_output + v) // 2**20} MiB\")\n",
    "print(\"=\" * 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d70deac-a86a-403d-a21b-097e77e932fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.000\n",
      "･ CPU:          0          0      3,057 MB |\n",
      "･ GPU:          0          0      7,404 MB |\n"
     ]
    }
   ],
   "source": [
    "# https://arxiv.org/pdf/2205.05198.pdf\n",
    "\n",
    "def calculate_attention_block():\n",
    "     return 11 * seq_length * bs * hidden_size + 5 * num_attention_heads * seq_length ** 2 * bs\n",
    "\n",
    "def calculate_mlp_block():\n",
    "     return 19 * seq_length * bs * hidden_size\n",
    "\n",
    "def calculate_layernorms():\n",
    "    return 4 * seq_length * bs * hidden_size\n",
    "\n",
    "def calculate_per_layer():\n",
    "    return seq_length * bs * hidden_size * (34 + 5 * num_attention_heads * seq_length / hidden_size)\n",
    "\n",
    "assert calculate_attention_block() + calculate_mlp_block() + calculate_layernorms() == calculate_per_layer() == layer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23628603-07f8-4f3f-9731-018939acf519",
   "metadata": {},
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
