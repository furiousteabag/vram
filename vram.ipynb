{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8e3cebc7-369d-4cf0-b9bf-63555f042bb2",
   "metadata": {},
   "source": [
    "pip install transformers nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d0272a-78b0-48ac-b6d1-e7b57dc01650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import pynvml\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "n_bytes_per_param = 2 if dtype in (torch.float16, torch.bfloat16) else 4\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "get_vram = lambda: pynvml.nvmlDeviceGetMemoryInfo(handle).used / 2**20 # MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d48830c-cd59-489b-b500-459eb647c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA kernels VRAM: 361 MiB\n"
     ]
    }
   ],
   "source": [
    "start_vram = get_vram()\n",
    "\n",
    "# Initializing CUDA kernels\n",
    "a = torch.ones((1,1)).to(device); del a\n",
    "cuda_kernels_vram = get_vram() - start_vram\n",
    "print(f\"CUDA kernels VRAM: {cuda_kernels_vram:.0f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4b460a-a665-4b34-8010-6e3343df4c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b6118afa324b249a31cf5e016c0a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 6.738 B\n",
      "Estimated VRAM usage: 12.551 GiB\n",
      "Actual VRAM usage: 12.613 GiB\n",
      "Error: 0.5 %\n"
     ]
    }
   ],
   "source": [
    "#model_name_or_path = \"gpt2-xl\" # gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "#model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_name_or_path = \"NousResearch/Llama-2-7b-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters())\n",
    "model_estimated_vram = n_parameters * n_bytes_per_param / 2**20\n",
    "model_actual_vram = get_vram() - cuda_kernels_vram - start_vram\n",
    "\n",
    "display(model)\n",
    "print(f\"Number of parameters: {(n_parameters / 1e9):.3f} B\")\n",
    "print(f\"Estimated VRAM usage: {(model_estimated_vram / 2**10):.3f} GiB\")\n",
    "print(f\"Actual VRAM usage: {(model_actual_vram / 2**10):.3f} GiB\")\n",
    "print(f\"Error: {((model_actual_vram - model_estimated_vram) * 100 / model_actual_vram):.1f} %\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69cd292a-9e1b-4279-968d-c60099c07063",
   "metadata": {},
   "source": [
    "bs = 128\n",
    "seq_length = 8192\n",
    "\n",
    "batch_vram = 3 * bs * seq_length * 8 # 3 for input_ids, attention_masks and labels; 8 for each i64\n",
    "print(f\"For batch of {128} items with a sequence length of {seq_length} it will consume {batch_vram / 2**20} MiB VRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5762f438-77e1-4c6d-a3c9-7eaeab21dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8\n",
    "seq_length = 1024\n",
    "\n",
    "input_ids = torch.randint(0, len(tokenizer), (bs, seq_length)).to(device)\n",
    "attention_mask = torch.ones((bs, seq_length)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733503d2-e9de-4122-93a2-eb7ca86f87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.eval()\n",
    "model.config.use_cache = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "out_bs, out_sequence_length, out_embedding_size = out.logits.shape\n",
    "n_bytes_per_param_out = 2 if out.logits.dtype in (torch.float16, torch.bfloat16) else 4\n",
    "\n",
    "#print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e17df629-7efa-4a73-bdca-2f264d1eef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total forward pass VRAM usage: 3764 MiB\n",
      "Actual output tensor with bs 8, seq length 1024 and emb size 32000 VRAM usage: 1024 MiB\n",
      "Estimated output tensor with bs 8, seq length 1024 and emb size 32000 VRAM usage: 1000 MiB\n",
      "Actual activations VRAM usage: 2708 MiB\n"
     ]
    }
   ],
   "source": [
    "total_forward_pass_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "print(f\"Total forward pass VRAM usage: {total_forward_pass_vram:.0f} MiB\")\n",
    "\n",
    "torch.cuda.empty_cache() # calling `free` on allocated memory for forward pass\n",
    "output_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "del out\n",
    "torch.cuda.empty_cache() # calling `free` on allocated memory for `out` tensor\n",
    "eps_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram # idk what is that, but it is small\n",
    "\n",
    "output_actual_vram = output_vram - eps_vram\n",
    "activations_actual_vram = total_forward_pass_vram - output_actual_vram - eps_vram\n",
    "\n",
    "output_estimated_vram = out_bs * out_sequence_length * out_embedding_size * n_bytes_per_param_out / 2**20\n",
    "\n",
    "print(f\"Actual output tensor with bs {out_bs}, seq length {out_sequence_length} and emb size {out_embedding_size} VRAM usage: {output_actual_vram:.0f} MiB\")\n",
    "print(f\"Estimated output tensor with bs {out_bs}, seq length {out_sequence_length} and emb size {out_embedding_size} VRAM usage: {output_estimated_vram:.0f} MiB\")\n",
    "print(f\"Actual activations VRAM usage: {activations_actual_vram:.0f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca072747-a715-4827-a4a1-3335b9c844a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating size of activation for single block with:\n",
      "hidden size 4096\n",
      "num attention heads 32\n",
      "num key value heads 32\n",
      "intermediate size 11008\n",
      "head dim 128\n",
      "Single layer (out of 32) estimated activations VRAM usage: 1368 MiB\n",
      "Estimated activations VRAM usage (softmax output + v): 576 MiB\n"
     ]
    }
   ],
   "source": [
    "hidden_size = model.config.hidden_size\n",
    "num_attention_heads = model.config.num_attention_heads\n",
    "num_key_value_heads = model.config.num_key_value_heads # different from num_attention_heads in case of GQA\n",
    "intermediate_size = model.config.intermediate_size # MLP projection\n",
    "head_dim = hidden_size // num_attention_heads\n",
    "print(f\"Calculating size of activation for single block with:\\nhidden size {hidden_size}\\nnum attention heads {num_attention_heads}\\nnum key value heads {num_key_value_heads}\\nintermediate size {intermediate_size}\\nhead dim {head_dim}\")\n",
    "\n",
    "attention_input      = n_bytes_per_param * bs * seq_length * hidden_size\n",
    "q                    = n_bytes_per_param * bs * seq_length * head_dim * num_attention_heads # for Q @ K.T\n",
    "k                    = n_bytes_per_param * bs * seq_length * head_dim * num_key_value_heads # num_key_value_heads might be different from num_attention_heads in case of GQA\n",
    "softmax_output       = n_bytes_per_param * bs * num_attention_heads * seq_length ** 2 # to multiply with V\n",
    "#softmax_dropout_mask = 1                 * bs * num_attention_heads * seq_length ** 2 # single byte per elem\n",
    "#dropout_output       = n_bytes_per_param * bs * num_attention_heads * seq_length ** 2\n",
    "v                    = n_bytes_per_param * bs * seq_length * head_dim * num_key_value_heads\n",
    "out_proj_input       = n_bytes_per_param * bs * seq_length * num_attention_heads * head_dim\n",
    "#attention_dropout    = 1                 * bs * seq_length * hidden_size\n",
    "attention_block = attention_input + q + k + softmax_output + v + out_proj_input\n",
    "\n",
    "mlp_input        = n_bytes_per_param * bs * seq_length * hidden_size\n",
    "activation_input = n_bytes_per_param * bs * seq_length * intermediate_size # SiLU\n",
    "down_proj_input  = n_bytes_per_param * bs * seq_length * intermediate_size\n",
    "mlp_block = mlp_input + activation_input + down_proj_input\n",
    "\n",
    "layer_norms = n_bytes_per_param * bs * seq_length * hidden_size * 2 # 2 layer norms\n",
    "\n",
    "layer = attention_block + mlp_block + layer_norms\n",
    "print(f\"Single layer (out of {model.config.num_hidden_layers}) estimated activations VRAM usage: {layer // 2**20} MiB\")\n",
    "print(f\"Estimated activations VRAM usage (softmax output + v): {(softmax_output + v) // 2**20} MiB\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a22e356d-cd17-4368-b0ed-ad018c2b0a5d",
   "metadata": {},
   "source": [
    "11 * seq_length * bs * hidden_size + 5 * num_attention_heads * seq_length ** 2 * bs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23628603-07f8-4f3f-9731-018939acf519",
   "metadata": {},
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
