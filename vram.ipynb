{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8e3cebc7-369d-4cf0-b9bf-63555f042bb2",
   "metadata": {},
   "source": [
    "pip install transformers nvidia-ml-py3 einops ipyexperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d0272a-78b0-48ac-b6d1-e7b57dc01650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 0, NVIDIA GeForce RTX 3090 (24576 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:     Used     Free    Total        Util\n",
      "CPU:    1,605  236,550  257,616 MB   0.62% \n",
      "GPU:    1,031   23,544   24,576 MB   4.20% \n",
      "\n",
      "\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.000\n",
      "･ CPU:          0          0      1,605 MB |\n",
      "･ GPU:          0          0      1,031 MB |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import pynvml\n",
    "from ipyexperiments import IPyExperimentsPytorch\n",
    "exp1 = IPyExperimentsPytorch()\n",
    "\n",
    "dtype = torch.float16\n",
    "n_bytes_per_param = 2 if dtype in (torch.float16, torch.bfloat16) else 4\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "get_vram = lambda: pynvml.nvmlDeviceGetMemoryInfo(handle).used / 2**20 # MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d48830c-cd59-489b-b500-459eb647c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA kernels VRAM: 2 MiB\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.001\n",
      "･ CPU:          0          0      1,605 MB |\n",
      "･ GPU:          0          2      1,031 MB |\n"
     ]
    }
   ],
   "source": [
    "start_vram = get_vram()\n",
    "\n",
    "# Initializing CUDA kernels\n",
    "a = torch.ones((1,1)).to(device); del a\n",
    "cuda_kernels_vram = get_vram() - start_vram\n",
    "print(f\"CUDA kernels VRAM: {cuda_kernels_vram:.0f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1689d757-e854-45b8-a35d-3e6e31994b83",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4b460a-a665-4b34-8010-6e3343df4c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiConfig {\n",
      "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"PhiForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_phi.PhiConfig\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_phi.PhiForCausalLM\"\n",
      "  },\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"flash_attn\": false,\n",
      "  \"flash_rotary\": false,\n",
      "  \"fused_dense\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"phi-msft\",\n",
      "  \"n_embd\": 2048,\n",
      "  \"n_head\": 32,\n",
      "  \"n_head_kv\": null,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary_dim\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "PhiForCausalLM(\n",
      "  (transformer): PhiModel(\n",
      "    (embd): Embedding(\n",
      "      (wte): Embedding(51200, 2048)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x ParallelBlock(\n",
      "        (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (mixer): MHA(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (inner_attn): SelfAttention(\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (inner_cross_attn): CrossAttention(\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): CausalLMHead(\n",
      "    (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear): Linear(in_features=2048, out_features=51200, bias=True)\n",
      "  )\n",
      "  (loss): CausalLMLoss(\n",
      "    (loss_fct): CrossEntropyLoss()\n",
      "  )\n",
      ")\n",
      "Number of parameters: 1.418 B (1418270720)\n",
      "Estimated VRAM usage: 2705.137 MiB\n",
      "Actual VRAM usage: 2752.000 MiB\n",
      "Error: 1.7 %\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:38.373\n",
      "･ CPU:        397      5,022      2,003 MB |\n",
      "･ GPU:      2,754          0      3,785 MB |\n"
     ]
    }
   ],
   "source": [
    "#model_name_or_path = \"gpt2-xl\" # gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
    "#model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "#model_name_or_path = \"NousResearch/Llama-2-7b-hf\"\n",
    "model_name_or_path = \"microsoft/phi-1_5\" # phi-1_5, phi-2\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=dtype, trust_remote_code=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters())\n",
    "model_estimated_vram = n_parameters * n_bytes_per_param / 2**20\n",
    "model_actual_vram = get_vram() - cuda_kernels_vram - start_vram\n",
    "\n",
    "print(model.config)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {(n_parameters / 1e9):.3f} B ({n_parameters})\")\n",
    "print(f\"Estimated VRAM usage: {(model_estimated_vram):.3f} MiB\")\n",
    "print(f\"Actual VRAM usage: {(model_actual_vram):.3f} MiB\")\n",
    "print(f\"Error: {((model_actual_vram - model_estimated_vram) * 100 / model_actual_vram):.1f} %\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69cd292a-9e1b-4279-968d-c60099c07063",
   "metadata": {},
   "source": [
    "bs = 128\n",
    "seq_length = 8192\n",
    "\n",
    "batch_vram = 3 * bs * seq_length * 8 # 3 for input_ids, attention_masks and labels; 8 for each i64\n",
    "print(f\"For batch of {bs} items with a sequence length of {seq_length} it will consume {batch_vram / 2**20} MiB VRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5762f438-77e1-4c6d-a3c9-7eaeab21dd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.005\n",
      "･ CPU:          0          0      2,003 MB |\n",
      "･ GPU:          0          0      3,785 MB |\n"
     ]
    }
   ],
   "source": [
    "bs = 2\n",
    "seq_length = 128\n",
    "\n",
    "input_ids = torch.randint(0, len(tokenizer), (bs, seq_length)).to(device)\n",
    "attention_mask = torch.ones((bs, seq_length)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b004db1-7540-47c7-a498-aa8f43b910b6",
   "metadata": {},
   "source": [
    "# Inference forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733503d2-e9de-4122-93a2-eb7ca86f87fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out tensor dtype: torch.float32\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.937\n",
      "･ CPU:        652          0      2,656 MB |\n",
      "･ GPU:        430         34      4,215 MB |\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "_ = model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    # probs = F.softmax(out.logits[:, -1, :], dim=-1) # for inference we need probabilities only over the last token; omit this as it is very small\n",
    "    \n",
    "out_bs, out_sequence_length, out_embedding_size = out.logits.shape\n",
    "n_bytes_per_param_out = 2 if out.logits.dtype in (torch.float16, torch.bfloat16) else 4\n",
    "print(f\"Out tensor dtype: {out.logits.dtype}\")\n",
    "\n",
    "#print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e410534-cc46-42a0-b1c2-20dc39ebe6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total forward pass VRAM usage: 430 MiB\n",
      "Actual output tensor with bs 2, seq length 128 and emb size 51200 VRAM usage: 50 MiB\n",
      "Estimated output tensor with bs 2, seq length 128 and emb size 51200 VRAM usage: 50 MiB\n",
      "Actual activations VRAM usage: 0 MiB\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.001\n",
      "･ CPU:          0          0      2,656 MB |\n",
      "･ GPU:        -50          0      4,165 MB |\n"
     ]
    }
   ],
   "source": [
    "total_forward_pass_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "print(f\"Total forward pass VRAM usage: {total_forward_pass_vram:.0f} MiB\")\n",
    "\n",
    "torch.cuda.empty_cache() # calling `free` on allocated memory for forward pass\n",
    "output_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram\n",
    "del out\n",
    "torch.cuda.empty_cache() # calling `free` on allocated memory for `out` tensor\n",
    "eps_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram # idk what is that, but it is small\n",
    "\n",
    "output_actual_vram = output_vram - eps_vram\n",
    "activations_actual_vram = total_forward_pass_vram - output_actual_vram - eps_vram\n",
    "\n",
    "output_estimated_vram = out_bs * out_sequence_length * out_embedding_size * n_bytes_per_param_out / 2**20\n",
    "\n",
    "print(f\"Actual output tensor with bs {out_bs}, seq length {out_sequence_length} and emb size {out_embedding_size} VRAM usage: {output_actual_vram:.0f} MiB\")\n",
    "print(f\"Estimated output tensor with bs {out_bs}, seq length {out_sequence_length} and emb size {out_embedding_size} VRAM usage: {output_estimated_vram:.0f} MiB\")\n",
    "print(f\"Actual activations VRAM usage: {activations_actual_vram:.0f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f8bac-c9a4-42b5-bad8-3f5e41fba771",
   "metadata": {},
   "source": [
    "# Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292d081-a0df-4923-ae9f-db31503b0e50",
   "metadata": {},
   "source": [
    "## no optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f160dd99-a5b0-4f23-aa95-d003734ff2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train forward pass VRAM usage (with output tensor): 878 MiB\n",
      "Actual probs tensor VRAM usage: 50 MiB\n",
      "Loss calculation VRAM usage: 100 MiB\n",
      "Backward calculation VRAM usage: 2468 MiB\n",
      "Gradients type: torch.float16\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.193\n",
      "･ CPU:          6          0      2,662 MB |\n",
      "･ GPU:      2,876        620      7,041 MB |\n"
     ]
    }
   ],
   "source": [
    "_ = model.train()\n",
    "out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "total_train_forward_pass_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Total train forward pass VRAM usage (with output tensor): {total_train_forward_pass_vram:.0f} MiB\")\n",
    "\n",
    "probs = F.softmax(out.logits, dim=-1)\n",
    "probs_vram = get_vram() - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Actual probs tensor VRAM usage: {probs_vram:.0f} MiB\")\n",
    "\n",
    "loss = F.cross_entropy(probs.permute(0, 2, 1), input_ids) # mapping tokens into themselves\n",
    "loss_calculation_vram = get_vram() - probs_vram - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Loss calculation VRAM usage: {loss_calculation_vram:.0f} MiB\")\n",
    "loss.backward()\n",
    "backward_vram = get_vram() - loss_calculation_vram - probs_vram - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Backward calculation VRAM usage: {backward_vram:.0f} MiB\")\n",
    "\n",
    "print(f\"Gradients type: {next(model.parameters()).grad.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a32c58-fe76-4d75-b6ed-8d9c7447de25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual gradients VRAM usage: 2718 MiB\n",
      "Random eps VRAM usage: 438 MiB\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.084\n",
      "･ CPU:          0          0      2,662 MB |\n",
      "･ GPU:     -2,818          0      4,223 MB |\n"
     ]
    }
   ],
   "source": [
    "del out\n",
    "del probs\n",
    "del loss\n",
    "\n",
    "torch.cuda.empty_cache() # calling `free` on allocated memory for activations\n",
    "gradients_total_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "model.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "eps_2_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "gradients_actual_vram = gradients_total_vram - eps_2_vram\n",
    "print(f\"Actual gradients VRAM usage: {gradients_actual_vram:.0f} MiB\")\n",
    "\n",
    "eps_vram += eps_2_vram\n",
    "print(f\"Random eps VRAM usage: {eps_vram:.0f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870854c0-ea8d-41c1-96d4-f2f3545b045f",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66b0bb-7976-44c5-8086-a7025bb20878",
   "metadata": {},
   "source": [
    "### no momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0958e7a6-8a1d-4a26-afda-9fdbfe3423a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting to use 0 extra VRAM: 0.0\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.003\n",
      "･ CPU:          0          0      2,662 MB |\n",
      "･ GPU:          0          0      4,223 MB |\n"
     ]
    }
   ],
   "source": [
    "print(f\"Expecting to use 0 extra VRAM: {get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram}\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "_ = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c784a400-c531-4029-8ec1-37158124b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train forward pass VRAM usage (with output tensor): 838 MiB\n",
      "Actual probs tensor VRAM usage: 50 MiB\n",
      "Loss calculation VRAM usage: 100 MiB\n",
      "Backward calculation VRAM usage: 2436 MiB\n",
      "Gradients type: torch.float16\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.197\n",
      "･ CPU:          0          0      2,662 MB |\n",
      "･ GPU:      2,804        620      7,027 MB |\n"
     ]
    }
   ],
   "source": [
    "out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "total_train_forward_pass_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Total train forward pass VRAM usage (with output tensor): {total_train_forward_pass_vram:.0f} MiB\")\n",
    "\n",
    "probs = F.softmax(out.logits, dim=-1)\n",
    "probs_vram = get_vram() - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Actual probs tensor VRAM usage: {probs_vram:.0f} MiB\")\n",
    "\n",
    "loss = F.cross_entropy(probs.permute(0, 2, 1), input_ids) # mapping tokens into themselves\n",
    "loss_calculation_vram = get_vram() - probs_vram - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Loss calculation VRAM usage: {loss_calculation_vram:.0f} MiB\")\n",
    "loss.backward()\n",
    "backward_vram = get_vram() - loss_calculation_vram - probs_vram - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Backward calculation VRAM usage: {backward_vram:.0f} MiB\")\n",
    "\n",
    "optimizer.step()\n",
    "#optim_step_vram = get_vram() - backward_vram - loss_calculation_vram - probs_vram - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "#print(f\"Optim step VRAM usage: {optim_step_vram:.0f} MiB\")\n",
    "\n",
    "print(f\"Gradients type: {next(model.parameters()).grad.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae24306d-3d65-44c6-ac8a-1dcbe7cd7250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual gradients VRAM usage: 2704 MiB\n",
      "Random eps VRAM usage: 438 MiB\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.080\n",
      "･ CPU:          0          0      2,662 MB |\n",
      "･ GPU:     -2,804          0      4,223 MB |\n"
     ]
    }
   ],
   "source": [
    "del out\n",
    "del probs\n",
    "del loss\n",
    "\n",
    "torch.cuda.empty_cache() # calling `free` on allocated memory for activations\n",
    "gradients_total_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "eps_2_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "gradients_actual_vram = gradients_total_vram - eps_2_vram\n",
    "print(f\"Actual gradients VRAM usage: {gradients_actual_vram:.0f} MiB\")\n",
    "\n",
    "eps_vram += eps_2_vram\n",
    "print(f\"Random eps VRAM usage: {eps_vram:.0f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ada51f-7b1c-435d-be79-5d4456e8571e",
   "metadata": {},
   "source": [
    "### momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b6d7f2d-b6a6-4c50-849e-e27af2ea19aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting to use 0 extra VRAM: 0.0\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.003\n",
      "･ CPU:          0          0      2,662 MB |\n",
      "･ GPU:          0          0      4,223 MB |\n"
     ]
    }
   ],
   "source": [
    "print(f\"Expecting to use 0 extra VRAM: {get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram}\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "_ = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ea0a0a7-6ee9-490b-9f05-b0780b75ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train forward pass VRAM usage (with output tensor): 838 MiB\n",
      "Actual probs tensor VRAM usage: 50 MiB\n",
      "Loss calculation VRAM usage: 100 MiB\n",
      "Backward calculation VRAM usage: 2436 MiB\n",
      "Gradients type: torch.float16\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.216\n",
      "･ CPU:          0          0      2,662 MB |\n",
      "･ GPU:      5,556        380      9,779 MB |\n"
     ]
    }
   ],
   "source": [
    "out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "total_train_forward_pass_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Total train forward pass VRAM usage (with output tensor): {total_train_forward_pass_vram:.0f} MiB\")\n",
    "\n",
    "probs = F.softmax(out.logits, dim=-1)\n",
    "probs_vram = get_vram() - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Actual probs tensor VRAM usage: {probs_vram:.0f} MiB\")\n",
    "\n",
    "loss = F.cross_entropy(probs.permute(0, 2, 1), input_ids) # mapping tokens into themselves\n",
    "loss_calculation_vram = get_vram() - probs_vram - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Loss calculation VRAM usage: {loss_calculation_vram:.0f} MiB\")\n",
    "loss.backward()\n",
    "backward_vram = get_vram() - loss_calculation_vram - probs_vram - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Backward calculation VRAM usage: {backward_vram:.0f} MiB\")\n",
    "\n",
    "optimizer.step()\n",
    "#optim_step_vram = get_vram() - backward_vram - loss_calculation_vram - probs_vram - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "#print(f\"Optim step VRAM usage: {optim_step_vram:.0f} MiB\")\n",
    "\n",
    "print(f\"Gradients type: {next(model.parameters()).grad.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5c48402-6766-4c0b-a6d5-ed6cd7ce5156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual gradients VRAM usage: 2702 MiB\n",
      "Actual optimizer states VRAM usage: 2754 MiB\n",
      "Random eps VRAM usage: 438 MiB\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.168\n",
      "･ CPU:          0          0      2,662 MB |\n",
      "･ GPU:     -5,556          0      4,223 MB |\n"
     ]
    }
   ],
   "source": [
    "del out\n",
    "del probs\n",
    "del loss\n",
    "\n",
    "torch.cuda.empty_cache() # calling `free` on allocated memory for activations\n",
    "gradients_optimizer_total_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "optimizer_total_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()\n",
    "eps_2_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "gradients_actual_vram = gradients_optimizer_total_vram - optimizer_total_vram\n",
    "optimizer_actual_vram = optimizer_total_vram - eps_2_vram\n",
    "print(f\"Actual gradients VRAM usage: {gradients_actual_vram:.0f} MiB\")\n",
    "print(f\"Actual optimizer states VRAM usage: {optimizer_actual_vram:.0f} MiB\")\n",
    "\n",
    "eps_vram += eps_2_vram\n",
    "print(f\"Random eps VRAM usage: {eps_vram:.0f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0ff0b-7772-4c1e-bc25-8a41336e3d95",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fef0304-f840-4039-9c11-41440683df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting to use 0 extra VRAM: 0.0\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.003\n",
      "･ CPU:          0          0      2,662 MB |\n",
      "･ GPU:          0          0      4,223 MB |\n"
     ]
    }
   ],
   "source": [
    "print(f\"Expecting to use 0 extra VRAM: {get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram}\")\n",
    "get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\n",
    "_ = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d03b970-ceab-4ea2-abfb-931166de1258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train forward pass VRAM usage (with output tensor): 838 MiB\n",
      "Actual probs tensor VRAM usage: 50 MiB\n",
      "Loss calculation VRAM usage: 100 MiB\n",
      "Backward calculation VRAM usage: 2436 MiB\n",
      "Gradients type: torch.float16\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.311\n",
      "･ CPU:          1          0      2,664 MB |\n",
      "･ GPU:      8,310      5,602     12,533 MB |\n"
     ]
    }
   ],
   "source": [
    "out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "total_train_forward_pass_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Total train forward pass VRAM usage (with output tensor): {total_train_forward_pass_vram:.0f} MiB\")\n",
    "\n",
    "probs = F.softmax(out.logits, dim=-1)\n",
    "probs_vram = get_vram() - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Actual probs tensor VRAM usage: {probs_vram:.0f} MiB\")\n",
    "\n",
    "loss = F.cross_entropy(probs.permute(0, 2, 1), input_ids) # mapping tokens into themselves\n",
    "loss_calculation_vram = get_vram() - probs_vram - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Loss calculation VRAM usage: {loss_calculation_vram:.0f} MiB\")\n",
    "loss.backward()\n",
    "backward_vram = get_vram() - loss_calculation_vram - probs_vram - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "print(f\"Backward calculation VRAM usage: {backward_vram:.0f} MiB\")\n",
    "\n",
    "optimizer.step()\n",
    "#optim_step_vram = get_vram() - backward_vram - loss_calculation_vram - probs_vram - total_train_forward_pass_vram - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "#print(f\"Optim step VRAM usage: {optim_step_vram:.0f} MiB\")\n",
    "\n",
    "print(f\"Gradients type: {next(model.parameters()).grad.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48082459-7354-485f-bbf4-b04c868f0854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 15 08:36:28 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  N/A |\n",
      "| 30%   42C    P2   128W / 350W |  12217MiB / 24576MiB |     13%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.403\n",
      "･ CPU:          0          0      2,665 MB |\n",
      "･ GPU:          0          0     12,533 MB |\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7bf49d0-eb53-4294-94e9-c90839581ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual gradients VRAM usage: 2702 MiB\n",
      "Actual optimizer states VRAM usage: 5508 MiB\n",
      "Random eps VRAM usage: 438 MiB\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.239\n",
      "･ CPU:          0          0      2,665 MB |\n",
      "･ GPU:     -8,310          0      4,223 MB |\n"
     ]
    }
   ],
   "source": [
    "del out\n",
    "del probs\n",
    "del loss\n",
    "\n",
    "torch.cuda.empty_cache() # calling `free` on allocated memory for activations\n",
    "gradients_optimizer_total_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "optimizer_total_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()\n",
    "eps_2_vram = get_vram() - model_actual_vram - cuda_kernels_vram - start_vram - eps_vram\n",
    "gradients_actual_vram = gradients_optimizer_total_vram - optimizer_total_vram\n",
    "optimizer_actual_vram = optimizer_total_vram - eps_2_vram\n",
    "print(f\"Actual gradients VRAM usage: {gradients_actual_vram:.0f} MiB\")\n",
    "print(f\"Actual optimizer states VRAM usage: {optimizer_actual_vram:.0f} MiB\")\n",
    "\n",
    "eps_vram += eps_2_vram\n",
    "print(f\"Random eps VRAM usage: {eps_vram:.0f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "365631fd-06a2-4aa5-97a7-916dc66e52af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 15 08:36:29 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  N/A |\n",
      "| 30%   42C    P2   110W / 350W |   3907MiB / 24576MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.405\n",
      "･ CPU:          0          0      2,665 MB |\n",
      "･ GPU:          0          0      4,223 MB |\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd62ab9-22e3-4e84-943e-82f4b63762a1",
   "metadata": {},
   "source": [
    "# Estimation activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca072747-a715-4827-a4a1-3335b9c844a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating size of activation for single block with:\n",
      "hidden size 2048\n",
      "num attention heads 32\n",
      "num key value heads 32\n",
      "intermediate size 8192\n",
      "head dim 64\n",
      "Single layer (out of 24) estimated activations VRAM usage: 18 MiB\n",
      "Estimated activations VRAM usage (softmax output + v): 3 MiB\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.001\n",
      "･ CPU:          0          0      2,665 MB |\n",
      "･ GPU:          0          0      4,223 MB |\n"
     ]
    }
   ],
   "source": [
    "hidden_size = model.config.hidden_size\n",
    "num_attention_heads = model.config.num_attention_heads\n",
    "num_key_value_heads = model.config.num_key_value_heads if hasattr(model.config, \"num_key_value_heads\") else model.config.num_attention_heads # different from num_attention_heads in case of GQA\n",
    "intermediate_size = model.config.intermediate_size if hasattr(model.config, \"intermediate_size\") else 4 * model.config.hidden_size # MLP projection\n",
    "head_dim = hidden_size // num_attention_heads\n",
    "print(f\"Calculating size of activation for single block with:\\nhidden size {hidden_size}\\nnum attention heads {num_attention_heads}\\nnum key value heads {num_key_value_heads}\\nintermediate size {intermediate_size}\\nhead dim {head_dim}\")\n",
    "\n",
    "attention_input      = n_bytes_per_param * bs * seq_length * hidden_size\n",
    "q                    = n_bytes_per_param * bs * seq_length * head_dim * num_attention_heads # for Q @ K.T\n",
    "k                    = n_bytes_per_param * bs * seq_length * head_dim * num_key_value_heads # num_key_value_heads might be different from num_attention_heads in case of GQA\n",
    "softmax_output       = n_bytes_per_param * bs * num_attention_heads * seq_length ** 2 # to multiply with V\n",
    "#softmax_dropout_mask = 1                 * bs * num_attention_heads * seq_length ** 2 # single byte per elem\n",
    "#dropout_output       = n_bytes_per_param * bs * num_attention_heads * seq_length ** 2\n",
    "v                    = n_bytes_per_param * bs * seq_length * head_dim * num_key_value_heads\n",
    "out_proj_input       = n_bytes_per_param * bs * seq_length * num_attention_heads * head_dim\n",
    "#attention_dropout    = 1                 * bs * seq_length * hidden_size\n",
    "attention_block = attention_input + q + k + softmax_output + v + out_proj_input\n",
    "\n",
    "mlp_input        = n_bytes_per_param * bs * seq_length * hidden_size\n",
    "activation_input = n_bytes_per_param * bs * seq_length * intermediate_size # SiLU\n",
    "down_proj_input  = n_bytes_per_param * bs * seq_length * intermediate_size\n",
    "mlp_block = mlp_input + activation_input + down_proj_input\n",
    "\n",
    "layer_norms = n_bytes_per_param * bs * seq_length * hidden_size * 2 # 2 layer norms\n",
    "\n",
    "layer = attention_block + mlp_block + layer_norms\n",
    "print(f\"Single layer (out of {model.config.num_hidden_layers}) estimated activations VRAM usage: {layer // 2**20} MiB\")\n",
    "print(f\"Estimated activations VRAM usage (softmax output + v): {(softmax_output + v) // 2**20} MiB\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a22e356d-cd17-4368-b0ed-ad018c2b0a5d",
   "metadata": {},
   "source": [
    "11 * seq_length * bs * hidden_size + 5 * num_attention_heads * seq_length ** 2 * bs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23628603-07f8-4f3f-9731-018939acf519",
   "metadata": {},
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
